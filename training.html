<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training Module Documentation</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        .method {
            margin: 20px 0;
            padding: 15px;
            background-color: #f5f5f5;
            border-radius: 5px;
        }
        .method h3 {
            margin-top: 0;
            color: #333;
        }
        code {
            background-color: #eee;
            padding: 2px 4px;
            border-radius: 3px;
        }
        pre {
            background-color: #f8f8f8;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <h1>Training Module Documentation</h1>
    <p>The training module handles model training using XGBoost in SageMaker.</p>

    <h2>Class: Train</h2>
    <div class="method">
        <h3>Constructor</h3>
        <pre>
def __init__(self, pipeline_session, role, pipeline_name, base_job_name, s3_base_path, preprocessor, region):
        </pre>
        <p>Initializes the training module, setting up parameters for model training.</p>
        <p>Parameters:</p>
        <ul>
            <li><code>pipeline_session</code>: SageMaker pipeline session</li>
            <li><code>role</code>: IAM role for execution</li>
            <li><code>pipeline_name</code>: Name of the pipeline</li>
            <li><code>base_job_name</code>: Base name for jobs</li>
            <li><code>s3_base_path</code>: S3 path for data storage</li>
            <li><code>preprocessor</code>: Preprocessor instance</li>
            <li><code>region</code>: AWS region</li>
        </ul>
    </div>

    <h2>How the Training Process Works</h2>
    <p>The training process in SageMaker follows these steps:</p>
    <ol>
        <li><strong>Instance Launch:</strong> A SageMaker training instance of the specified type (e.g., <code>ml.m5.4xlarge</code>) is started.</li>
        <li><strong>Mounting Training Data:</strong>
            <ul>
                <li>Training and validation data from S3 are mounted to the training container.</li>
                <li>The data is referenced using <code>TrainingInput</code> objects in SageMaker.</li>
                <li>
                    <b>***The following lines establish a dependency on the preprocessing step, ensuring that training will only execute once preprocessing is completed:***</b>
                    <pre>
's3_data': preprocessor.step.properties.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri
's3_data': preprocessor.step.properties.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri
                    </pre>
                    This mechanism ensures that the execution order of steps is maintained, enforcing the dependency between preprocessing and training.
                </li>
            </ul>
        </li>
        <li><strong>Training Execution:</strong>
            <ul>
                <li>The XGBoost training script is executed using SageMakerâ€™s built-in XGBoost container.</li>
                <li>Hyperparameters such as learning rate, depth, and subsample ratio are set before training begins.</li>
            </ul>
        </li>
        <li><strong>Model Output:</strong>
            <ul>
                <li>The trained model artifact is saved to S3 in the specified output path.</li>
            </ul>
        </li>
        <li><strong>Instance Termination:</strong> Once training is complete, the instance is automatically shut down.</li>
    </ol>

    <h2>Key Methods</h2>
    <div class="method">
        <h3>create_estimator()</h3>
        <p>Creates an XGBoost estimator with specified hyperparameters.</p>
        <pre>
def create_estimator(self):
        </pre>
    </div>

    <div class="method">
        <h3>create_step()</h3>
        <p>Creates a TuningStep for hyperparameter optimization.</p>
        <pre>
def create_step(self):
        </pre>
    </div>

    <h2>Hyperparameter Tuning</h2>
    <p>The module supports hyperparameter tuning with the following parameters:</p>
    <ul>
        <li>max_depth: 5</li>
        <li>gamma: 0</li>
        <li>learning_rate: 0.01</li>
        <li>min_child_weight: 6</li>
        <li>subsample: 0.8</li>
        <li>objective: binary:logistic</li>
        <li>eval_metric: aucpr</li>
    </ul>

    <a href="index.html">Back to Index</a>
</body>
</html>
